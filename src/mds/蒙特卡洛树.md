```javascript
{
"title":"蒙特卡洛树",
"category":"笔记",
"tags":["AI", "算法"],
"date":["2022","4","8"]
}
```

为了做AI课程的作业，需要用蒙特卡洛树的算法实现一个自动下棋的AI，然而无论是看书还是看课程PPT，或是网上的各类文章，还是觉得一知半解。于是也只好按照书上的框架先把代码给写完。结果是它只能保证打过随机函数，和初级难度对局好几局才能赢一次，更不用说其它难度了...（我严重怀疑是不是有些代码是不是压根就没跑起来，不过当时ddl将至也没时间细看了）

虽然但是，这个算法的原理还是差不多弄明白了，顺手记录一下好了。

首先是课件上给出的基本介绍，和网上查到的种种大同小异。

![蒙特卡洛树基本结构](blogPics/220408-1.jpg)

总觉得这类介绍应该是比较清楚了，但还是不算直观，还是想讲讲自己的理解吧。

首先，从概念上而言，蒙特卡洛树是希望通过对接下来每一个备选方案的结果随机采样的方式，评估下一步所有备选方案的优劣，也就意味着它会在规定步数或时间内不停采样、评估，知道外界告诉它停下。

具体操作上，将当前节点设为根节点，下一步所有的备选方案为它的子节点。接下来进入循环，不停选择备选方案并进行所谓的`模拟`和`反向传播`。其中选择备选方案的原则是：优先探索未探索过的节点，若全都探索过，就选择`UCB1`值更高的节点。在被迫结束时，返回`UCB1`值最高的方案并返回。

模拟的过程，就是根据被选中方案目前的状态随机继续进行直至结束，并返回结果。如果是下棋，即接下来从这个节点开始，两方都开始随机下棋，直到这句结束，并将结果（输、赢或平局）返回。

而反向传播，则是通过被选中节点以及它这次模拟返回的结果，对它和它以上节点的状态进行更新，更新内容包括反向传播次数N、总模拟奖励Q。其中每次奖励ΔQ需要自己设定，比方说棋局胜利为+1，败北为-1，平局为0。至于为什么要记录这两个值，就要讲到MCTS最重要的`UCB1`了。

`UCB1`的作用是为每一个节点进行打分，分数更高的节点将会更倾向于被探索和最后被选中返回。该函数需要读入该节点的`N`、`Q`和父节点的`N_0`，以及权重`C`，这个函数将会返回以下结果：

![](blogPics/220408-2.jpg)

其中`C`为权重，值越大越倾向于探索反向传播次数少的节点，可根据要求调整，我在作业中取值为根号二。在最终返回决策时调用UCB1计算时，一般将`C`置零。

需要注意这里只迭代了第一层，这里实际上可以继续向下拓展备选方案层，这样在下次调用时可以继承这次计算过程的一部分结果从而增加采样总数。每次的奖励也可以自己设定，例如让AI认为平局也挺好，就可以把平局的奖励也设为+1，或+0.5等。

这样就讲清楚了大致思路，但以上也只能算是我对MCTS算法的粗浅理解...若以后发现问题很大，在回来看看要不要改改...